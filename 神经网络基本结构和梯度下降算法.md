神经网络基本结构和梯度下降算法

![神经网络原理图](https://github.com/SuperrrWu/deep-learning/blob/master/Image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%95%E7%A4%BA%E5%9B%BE.jpg)

deep-learning的深度是指hidden layers

x:训练输入

y:训练输出

Cost-function（目标函数、损失函数）:

C(w,b)=Σx(1/2n)*abs(y(x)-t)

最小化问题可以使用梯度下降解决（gradient descent）

C(w,b)有两个变量，我们定义为V1，V2。

这是一个三维图像

通常可以通过微积分解决，如果v包含的变量过多，无法到达最低点。

一个变量的情况：假设一个小球在曲面上的某一点，滚动到最低点。

前提是：目标函数需要是凸函数。学习率表示每一步的大小，可能会有局部最优解。
对于梯度下降，我们需要计算对于每个x的梯度，因此当数据集过大时侯，计算速率会很慢。

所以我们有一个梯度下降算法(stochastic gradient descent)：

基本原理：从所有训练实例中取一个小取样（sample）：X1,X2,X3...Xm（mini-batch）

如果样本够大，则计算出来的梯度向量近似于正确。然后重新选择一个mini-batch，直到所有的训练实例，一轮epoch完成。
